{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ENLP A1 Part II: Perceptron\n",
    "\n",
    "Usage: python perceptron.py NITERATIONS\n",
    "\n",
    "(Adapted from Alan Ritter)\n",
    "\"\"\"\n",
    "import sys, os, glob\n",
    "\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from numpy import mean\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from evaluation import Eval\n",
    "\n",
    "from nbmodel import load_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_feats(doc):\n",
    "    \"\"\"\n",
    "    Extract input features (percepts) for a given document.\n",
    "    Each percept is a pairing of a name and a boolean, integer, or float value.\n",
    "    A document's percepts are the same regardless of the label considered.\n",
    "    \"\"\"\n",
    "    ff = Counter()\n",
    "    for word in doc:\n",
    "        ff[word] = 1\n",
    "    ff['bias_term'] = 1\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_featurized_docs(datasplit):\n",
    "    rawdocs, labels = load_docs(datasplit, lemmatize=False)\n",
    "    assert len(rawdocs)==len(labels)>0,datasplit\n",
    "    featdocs = []\n",
    "    for d in rawdocs:\n",
    "        featdocs.append(extract_feats(d))\n",
    "    return featdocs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CLASSES = ['ARA', 'DEU', 'FRA', 'HIN', 'ITA', 'JPN', 'KOR', 'SPA', 'TEL', 'TUR', 'ZHO']\n",
    "MAX_ITERATIONS = 30\n",
    "dev_docs,  dev_labels  = load_featurized_docs('dev')\n",
    "weights = {l: Counter() for l in CLASSES}\n",
    "#learn(train_docs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_docs, train_labels = load_featurized_docs('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_weights(self):\n",
    "    \"\"\"\n",
    "    Returns a copy of self.weights.\n",
    "    \"\"\"\n",
    "    return {l: Counter(c) for l,c in self.weights.items()}\n",
    "\n",
    "def learn(self, train_docs, train_labels):\n",
    "    \"\"\"\n",
    "    Train on the provided data with the perceptron algorithm.\n",
    "    Up to self.MAX_ITERATIONS of learning.\n",
    "    At the end of training, self.weights should contain the final model\n",
    "    parameters.\n",
    "    \"\"\"\n",
    "    for inter in self.MAX_ITERATIONS:\n",
    "        for i in range(train_docs):\n",
    "            for word in train_docs[i]:\n",
    "                \n",
    "    ...\n",
    "\n",
    "def score(self, doc, label):\n",
    "    \"\"\"\n",
    "    Returns the current model's score of labeling the given document\n",
    "    with the given label.\n",
    "    \"\"\"\n",
    "    return ...\n",
    "\n",
    "def predict(self, doc):\n",
    "    \"\"\"\n",
    "    Return the highest-scoring label for the document under the current model.\n",
    "    \"\"\"\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def test_eval(self, test_docs, test_labels):\n",
    "        pred_labels = [self.predict(d) for d in test_docs]\n",
    "        ev = Eval(test_labels, pred_labels)\n",
    "        return ev.accuracy()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    args = sys.argv[1:]\n",
    "    niters = int(args[0])\n",
    "\n",
    "    train_docs, train_labels = load_featurized_docs('train')\n",
    "    print(len(train_docs), 'training docs with',\n",
    "        sum(len(d) for d in train_docs)/len(train_docs), 'percepts on avg', file=sys.stderr)\n",
    "\n",
    "    dev_docs,  dev_labels  = load_featurized_docs('dev')\n",
    "    print(len(dev_docs), 'dev docs with',\n",
    "        sum(len(d) for d in dev_docs)/len(dev_docs), 'percepts on avg', file=sys.stderr)\n",
    "\n",
    "\n",
    "    test_docs,  test_labels  = load_featurized_docs('test')\n",
    "    print(len(test_docs), 'test docs with',\n",
    "        sum(len(d) for d in test_docs)/len(test_docs), 'percepts on avg', file=sys.stderr)\n",
    "\n",
    "    ptron = Perceptron(train_docs, train_labels, MAX_ITERATIONS=niters, dev_docs=dev_docs, dev_labels=dev_labels)\n",
    "    acc = ptron.test_eval(test_docs, test_labels)\n",
    "    print(acc, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5366 training docs with 154.68747670518076 percepts on avg\n",
      "598 dev docs with 154.81939799331104 percepts on avg\n",
      "604 test docs with 153.6341059602649 percepts on avg\n"
     ]
    }
   ],
   "source": [
    "train_docs, train_labels = load_featurized_docs('train')\n",
    "\n",
    "print(len(train_docs), 'training docs with',\n",
    "    sum(len(d) for d in train_docs)/len(train_docs), 'percepts on avg')\n",
    "\n",
    "dev_docs,  dev_labels  = load_featurized_docs('dev')\n",
    "print(len(dev_docs), 'dev docs with',\n",
    "    sum(len(d) for d in dev_docs)/len(dev_docs), 'percepts on avg')\n",
    "\n",
    "\n",
    "test_docs,  test_labels  = load_featurized_docs('test')\n",
    "print(len(test_docs), 'test docs with',\n",
    "    sum(len(d) for d in test_docs)/len(test_docs), 'percepts on avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ARA': Counter(),\n",
       " 'DEU': Counter(),\n",
       " 'FRA': Counter(),\n",
       " 'HIN': Counter(),\n",
       " 'ITA': Counter(),\n",
       " 'JPN': Counter(),\n",
       " 'KOR': Counter(),\n",
       " 'SPA': Counter(),\n",
       " 'TEL': Counter(),\n",
       " 'TUR': Counter(),\n",
       " 'ZHO': Counter()}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
