{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = []\n",
    "for i in range(0,5):\n",
    "    labels = [i]\n",
    "    label.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [1], [2], [3], [4]]\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, glob\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from evaluation import Eval\n",
    "labelMap = {}\n",
    "direc = \"train/\"\n",
    "labelMapFile='labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(direc, labelMapFile)) as inF:\n",
    "    for ln in inF:\n",
    "        docid, label = ln.strip().split(',')\n",
    "        assert docid not in labelMap\n",
    "        labelMap[docid] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs, labels = [], []\n",
    "\n",
    "for file_path in glob.glob(os.path.join(direc, '*.txt')):\n",
    "    filename = os.path.basename(file_path)\n",
    "    labels.append(labelMap[filename])\n",
    "    with open(file_path) as f:\n",
    "        docs.append(f.read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for key in labelMap.items():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#labelMap['1139492.txt'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#labelMap['1139492.txt'] +=1\n",
    "\n",
    "#value = '1139492.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#labelMap[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLASSES = ['ARA', 'DEU', 'FRA', 'HIN', 'ITA', 'JPN', 'KOR', 'SPA', 'TEL', 'TUR', 'ZHO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelCounts = {l: 0 for l in CLASSES}\n",
    "wordCounts = {l: Counter() for l in CLASSES}\n",
    "totalWordCounts = {l: 0 for l in CLASSES}\n",
    "trainVocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(labels)):\n",
    "    labelCounts[labels[i]] +=1\n",
    "    totalWordCounts[labels[i]] += len(docs[i])\n",
    "    words = docs[i]\n",
    "    wordCounts[labels[i]].update(words)\n",
    "    for word in words:\n",
    "    #    wordCounts[labels[i]].update(word)\n",
    "        trainVocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts['ARA']['ashfkashj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "priorProbs = {l: 0 for l in CLASSES}\n",
    "likelihoodProbs = {l: Counter() for l in CLASSES}\n",
    "ALPHA=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l in priorProbs:\n",
    "    priorProbs[l] = np.divide(labelCounts[l], len(labels))\n",
    "    \n",
    "    \n",
    "    for word in trainVocab: \n",
    "        likelihoodProbs[l][word] = np.divide(wordCounts[l][word]+ALPHA, totalWordCounts[l]+ALPHA*(len(trainVocab)+1))\n",
    "    likelihoodProbs[l]['**OOV**'] = np.divide(ALPHA, totalWordCounts[l]+ALPHA*(len(trainVocab)+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 1.0\n",
      "True 1.0\n",
      "True 0.999999999999\n",
      "True 1.0\n",
      "True 1.0\n",
      "True 1.0\n",
      "True 1.0\n",
      "True 1.0\n",
      "True 1.0\n",
      "True 1.0\n",
      "True 1.0\n"
     ]
    }
   ],
   "source": [
    "for y in CLASSES:\n",
    "    print(.999 < sum(likelihoodProbs[y].values()) < 1.001,sum(likelihoodProbs[y].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for word in likelihoodProbs['ARA']:\\n    print(likelihoodProbs['ARA'][word])\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for word in likelihoodProbs['ARA']:\n",
    "    print(likelihoodProbs['ARA'][word])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "j = 3\n",
    "for i in range(0,10):\n",
    "    if i == j:\n",
    "        break\n",
    "    else:\n",
    "        continue\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "lemmatize() missing 1 required positional argument: 'word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8100c21910ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'dogs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cats'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: lemmatize() missing 1 required positional argument: 'word'"
     ]
    }
   ],
   "source": [
    "l = ['dogs', 'cats']\n",
    "\n",
    "l = list(map(WordNetLemmatizer.lemmatize, l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(WordNetLemmatizer.lemmatize('dogs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(n, p):\n",
    "    if p:\n",
    "        n += p\n",
    "    return n\n",
    "    \n",
    "test(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.lemmatize('dogs'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
