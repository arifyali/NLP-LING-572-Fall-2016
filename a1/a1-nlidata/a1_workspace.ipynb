{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = []\n",
    "for i in range(0,5):\n",
    "    labels = [i]\n",
    "    label.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [1], [2], [3], [4]]\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, glob\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from evaluation import Eval\n",
    "labelMap = {}\n",
    "direc = \"train/\"\n",
    "labelMapFile='labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(direc, labelMapFile)) as inF:\n",
    "    for ln in inF:\n",
    "        docid, label = ln.strip().split(',')\n",
    "        assert docid not in labelMap\n",
    "        labelMap[docid] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs, labels = [], []\n",
    "\n",
    "for file_path in glob.glob(os.path.join(direc, '*.txt')):\n",
    "    filename = os.path.basename(file_path)\n",
    "    labels.append(labelMap[filename])\n",
    "    with open(file_path) as f:\n",
    "        doc = f.read().split()\n",
    "        if lemmatize:\n",
    "            docs.append([lm.lemmatize(word) for word in doc])\n",
    "        else:\n",
    "            docs.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'think',\n",
       " 'young',\n",
       " 'people',\n",
       " 'do',\n",
       " 'not',\n",
       " 'give',\n",
       " 'enough',\n",
       " 'time',\n",
       " 'to',\n",
       " 'helping',\n",
       " 'their',\n",
       " 'community',\n",
       " '.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'three',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'I',\n",
       " 'can',\n",
       " 'say',\n",
       " 'so',\n",
       " '.',\n",
       " 'First',\n",
       " ',',\n",
       " 'the',\n",
       " 'connection',\n",
       " 'between',\n",
       " 'people',\n",
       " 'in',\n",
       " 'our',\n",
       " 'society',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'getting',\n",
       " 'weaker',\n",
       " 'these',\n",
       " 'day',\n",
       " 'in',\n",
       " 'Japan',\n",
       " '.',\n",
       " 'Therefore',\n",
       " ',',\n",
       " 'it',\n",
       " 'doe',\n",
       " 'not',\n",
       " 'seem',\n",
       " 'that',\n",
       " 'we',\n",
       " 'have',\n",
       " 'to',\n",
       " 'pay',\n",
       " 'attention',\n",
       " 'to',\n",
       " 'the',\n",
       " 'community',\n",
       " 'any',\n",
       " 'more',\n",
       " '.',\n",
       " 'About',\n",
       " '40',\n",
       " 'or',\n",
       " '50',\n",
       " 'year',\n",
       " 'ago',\n",
       " ',',\n",
       " 'the',\n",
       " 'connection',\n",
       " 'between',\n",
       " 'people',\n",
       " 'in',\n",
       " 'the',\n",
       " 'community',\n",
       " 'wa',\n",
       " 'very',\n",
       " 'strong',\n",
       " '.',\n",
       " 'Because',\n",
       " 'there',\n",
       " 'are',\n",
       " 'le',\n",
       " 'people',\n",
       " 'in',\n",
       " 'one',\n",
       " 'community',\n",
       " ',',\n",
       " 'almost',\n",
       " 'all',\n",
       " 'the',\n",
       " 'member',\n",
       " 'of',\n",
       " 'community',\n",
       " 'knew',\n",
       " 'each',\n",
       " 'other',\n",
       " '.',\n",
       " 'Children',\n",
       " 'wa',\n",
       " 'scold',\n",
       " 'by',\n",
       " 'other',\n",
       " 'child',\n",
       " \"'s\",\n",
       " 'parent',\n",
       " 'if',\n",
       " 'they',\n",
       " 'behaved',\n",
       " 'badly',\n",
       " '.',\n",
       " 'So',\n",
       " ',',\n",
       " 'young',\n",
       " 'people',\n",
       " 'thought',\n",
       " 'they',\n",
       " 'were',\n",
       " 'nurtred',\n",
       " 'and',\n",
       " 'educated',\n",
       " 'by',\n",
       " 'the',\n",
       " 'community',\n",
       " '.',\n",
       " 'Therefore',\n",
       " 'they',\n",
       " 'tended',\n",
       " 'to',\n",
       " 'think',\n",
       " 'that',\n",
       " 'they',\n",
       " 'had',\n",
       " 'to',\n",
       " 'return',\n",
       " 'something',\n",
       " 'for',\n",
       " 'the',\n",
       " 'community',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'we',\n",
       " 'do',\n",
       " 'not',\n",
       " 'even',\n",
       " 'know',\n",
       " 'the',\n",
       " 'member',\n",
       " 'of',\n",
       " 'our',\n",
       " 'community',\n",
       " ',',\n",
       " 'because',\n",
       " 'there',\n",
       " 'are',\n",
       " 'too',\n",
       " 'many',\n",
       " 'people',\n",
       " 'in',\n",
       " 'one',\n",
       " 'community',\n",
       " '.',\n",
       " 'We',\n",
       " 'do',\n",
       " 'not',\n",
       " 'even',\n",
       " 'greet',\n",
       " 'each',\n",
       " 'other',\n",
       " '.',\n",
       " 'So',\n",
       " 'young',\n",
       " 'people',\n",
       " 'can',\n",
       " 'not',\n",
       " 'think',\n",
       " '``',\n",
       " 'we',\n",
       " 'are',\n",
       " 'here',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'good',\n",
       " 'community',\n",
       " \"''\",\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'young',\n",
       " 'people',\n",
       " 'do',\n",
       " 'not',\n",
       " 'give',\n",
       " 'enough',\n",
       " 'time',\n",
       " 'to',\n",
       " 'helping',\n",
       " 'their',\n",
       " 'community',\n",
       " '.',\n",
       " 'The',\n",
       " 'second',\n",
       " 'reason',\n",
       " 'is',\n",
       " 'that',\n",
       " 'young',\n",
       " 'people',\n",
       " 'spend',\n",
       " 'more',\n",
       " 'time',\n",
       " 'for',\n",
       " 'themselves',\n",
       " 'than',\n",
       " 'they',\n",
       " 'used',\n",
       " 'to',\n",
       " 'do',\n",
       " '.',\n",
       " 'We',\n",
       " 'have',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'entertainment',\n",
       " 'and',\n",
       " 'attractive',\n",
       " 'thing',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'nowadays',\n",
       " '.',\n",
       " 'Some',\n",
       " 'want',\n",
       " 'to',\n",
       " 'see',\n",
       " 'rock',\n",
       " 'musician',\n",
       " ',',\n",
       " 'others',\n",
       " 'want',\n",
       " 'to',\n",
       " 'see',\n",
       " 'baseball',\n",
       " 'game',\n",
       " '.',\n",
       " 'In',\n",
       " 'addition',\n",
       " ',',\n",
       " 'we',\n",
       " 'can',\n",
       " 'do',\n",
       " 'most',\n",
       " 'of',\n",
       " 'thing',\n",
       " 'without',\n",
       " 'anybody',\n",
       " \"'s\",\n",
       " 'help',\n",
       " 'in',\n",
       " 'the',\n",
       " 'community',\n",
       " '.',\n",
       " 'Althogh',\n",
       " 'some',\n",
       " 'young',\n",
       " 'people',\n",
       " 'are',\n",
       " 'helping',\n",
       " 'their',\n",
       " 'community',\n",
       " 'by',\n",
       " 'volunteering',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'now',\n",
       " 'it',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'work',\n",
       " 'for',\n",
       " 'their',\n",
       " 'own',\n",
       " 'purpose',\n",
       " '.',\n",
       " 'Many',\n",
       " 'university',\n",
       " 'and',\n",
       " 'company',\n",
       " 'admit',\n",
       " 'student',\n",
       " \"'\",\n",
       " 'volunteer',\n",
       " 'experience',\n",
       " 'a',\n",
       " 'their',\n",
       " '``',\n",
       " 'evidence',\n",
       " 'of',\n",
       " 'good',\n",
       " 'personality',\n",
       " 'and',\n",
       " 'social',\n",
       " 'skill',\n",
       " \"''\",\n",
       " 'So',\n",
       " ',',\n",
       " 'these',\n",
       " 'volunteer',\n",
       " 'work',\n",
       " 'are',\n",
       " 'done',\n",
       " 'for',\n",
       " 'their',\n",
       " 'own',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'the',\n",
       " 'community',\n",
       " '.',\n",
       " 'Most',\n",
       " 'of',\n",
       " 'young',\n",
       " 'people',\n",
       " 'may',\n",
       " 'not',\n",
       " 'do',\n",
       " 'volunteer',\n",
       " 'work',\n",
       " 'after',\n",
       " 'they',\n",
       " 'enter',\n",
       " 'the',\n",
       " 'university',\n",
       " 'or',\n",
       " 'company',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'why',\n",
       " 'I',\n",
       " 'think',\n",
       " 'young',\n",
       " 'people',\n",
       " 'do',\n",
       " 'not',\n",
       " 'give',\n",
       " 'enough',\n",
       " 'time',\n",
       " 'to',\n",
       " 'helping',\n",
       " 'their',\n",
       " 'community',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = docs[1]\n",
    "doc = [lm.lemmatize(word) for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-61605796cce3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#labelMap[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CLASSES = ['ARA', 'DEU', 'FRA', 'HIN', 'ITA', 'JPN', 'KOR', 'SPA', 'TEL', 'TUR', 'ZHO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelCounts = {l: 0 for l in CLASSES}\n",
    "wordCounts = {l: Counter() for l in CLASSES}\n",
    "totalWordCounts = {l: 0 for l in CLASSES}\n",
    "trainVocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(labels)):\n",
    "    labelCounts[labels[i]] +=1\n",
    "    totalWordCounts[labels[i]] += len(docs[i])\n",
    "    words = docs[i]\n",
    "    wordCounts[labels[i]].update(words)\n",
    "    for word in words:\n",
    "    #    wordCounts[labels[i]].update(word)\n",
    "        trainVocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts['ARA']['ashfkashj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "priorProbs = {l: 0 for l in CLASSES}\n",
    "likelihoodProbs = {l: Counter() for l in CLASSES}\n",
    "ALPHA=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l in priorProbs:\n",
    "    priorProbs[l] = np.divide(labelCounts[l], len(labels))\n",
    "    \n",
    "    \n",
    "    for word in trainVocab: \n",
    "        likelihoodProbs[l][word] = np.divide(wordCounts[l][word]+ALPHA, totalWordCounts[l]+ALPHA*(len(trainVocab)+1))\n",
    "    likelihoodProbs[l]['**OOV**'] = np.divide(ALPHA, totalWordCounts[l]+ALPHA*(len(trainVocab)+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 1.0\n",
      "True 1.0\n",
      "True 0.999999999999\n",
      "True 1.0\n",
      "True 1.0\n",
      "True 1.0\n",
      "True 1.0\n",
      "True 1.0\n",
      "True 1.0\n",
      "True 1.0\n",
      "True 1.0\n"
     ]
    }
   ],
   "source": [
    "for y in CLASSES:\n",
    "    print(.999 < sum(likelihoodProbs[y].values()) < 1.001,sum(likelihoodProbs[y].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for word in likelihoodProbs['ARA']:\\n    print(likelihoodProbs['ARA'][word])\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for word in likelihoodProbs['ARA']:\n",
    "    print(likelihoodProbs['ARA'][word])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "j = 3\n",
    "for i in range(0,10):\n",
    "    if i == j:\n",
    "        break\n",
    "    else:\n",
    "        continue\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "lemmatize() missing 1 required positional argument: 'word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8100c21910ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'dogs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cats'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: lemmatize() missing 1 required positional argument: 'word'"
     ]
    }
   ],
   "source": [
    "l = ['dogs', 'cats']\n",
    "\n",
    "l = list(map(WordNetLemmatizer.lemmatize, l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/arifali/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/Users/arifali/anaconda3/lib/python3.5/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpora/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arifali/anaconda3/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet.zip/wordnet/' not found.  Please use\n  the NLTK Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/arifali/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-b43734433f73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-b43734433f73>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/arifali/anaconda3/lib/python3.5/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arifali/anaconda3/lib/python3.5/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arifali/anaconda3/lib/python3.5/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpora/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arifali/anaconda3/lib/python3.5/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpora/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpora/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/arifali/anaconda3/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/arifali/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "results = ['cat', 'god']\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "lem = map(lemma.lemmatize, results)\n",
    "\n",
    "\n",
    "list(map(lambda x: lemma.lemmatize(x), results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2ec9b6f9981d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dogs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download()\n",
    "lemma.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(n, p):\n",
    "    if p:\n",
    "        n += p\n",
    "    return n\n",
    "    \n",
    "test(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.lemmatize('dogs'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
